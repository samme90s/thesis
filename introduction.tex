\section{Introduction}

This section introduces the different aspects of the research and to provide the
main scope of the paper.
It will describe large language models (LLMs) and their applications in text
classification.
Since we are collaborating with UPTILT that delivers a program to customers
within the service business, which solves tracking of materials and hourly
work to automate the delivery of invoices and offers to the customer.
Due to the collaboration, we will mainly discuss importance of classifying
and in the context of work orders (WOs), but also provide a general conclusion
on the topic and contribute to the field of text classification and natural
language processing (NLP) using LLMs.

\subsection{Background}

In many industries today, working with and handling large quantities of
structured and unstructured data is extremely important.
One example is text classification which labels text based on their content.
Traditionally, machine learning (ML) methods have been used for this purpose,
but recently, LLMs have shown great potential posing great generative
capabilities, along with understanding and reasoning within language(s)
\cite{huang2024} \cite{zhang2024}.

Training a ML model means analyzing the content of the text and identifying
features that are relevant, so computer systems can, for example, use this
technique for identifying spam and filtering documents \cite{dalal2011}.
Comparing this approach to an already widely trained LLM model, which does not
require the same manual labor and configuration to get going poses as a
competitive solution to our subject.
Therefore, we aim to further explore the usage of LLMs in relation with
classification within the area of WOs.

LLMs are a subgroup of AI that work on the knowledge of next word context and
are often utilized in the context of allowing the user to prompt it with
instructions or questions.
LLMs utilizes a much larger pre-trained knowledge compared to machine learning,
which often specializes in a specific domain and type of data.
LLMs can analyze data and recognize patterns with minimal human interaction,
and makes them highly effective in applications like NLP \cite{andersson2024}.

WOs is a document that outlines the details of a maintenance task.
This data comes in the form of text and can through proper utilization be the
key component when categorizing using LLMs.
WOs most commonly contains information regarding the material required to
complete such task, but also the geographic location, customer etc.
Today, many companies still rely on manual processing, which often leads to
mistakes.
Throughout our work we will focus on providing a better solution than the
current manual processing, by using LLMs \cite{ibm2023} \cite{li2024}.

\subsubsection{Related Work}

Huang and He \cite{huang2024} explain the cost of manual labor when it comes to
tagging and categorizing text.
Furthermore, the fine-tuning and choice of parameters can influence the final
behavior of the trained ML model.
They explain how the latest LLMs have showcased "remarkable reasoning
performance across a wide range of NLP tasks".
We aim to follow this trail and further analyze the efficiency and performance
of categorizing text within the context of WOs.

\bigskip
Recent advances in LLMs have shown that they can be fine-tuned to effectively
classify structured data.
For example, GPT-4, LLaMA 2, and ChatGLM 2 have shown remarkable performance in
text classification \cite{zhang2024}.
We will be discussing and evaluating the efficiency and accuracy of using LLMs
to classify structured WOs, unlike previous studies, which focuses more on
general NLP tasks.
Our paper gives an insight into whether LLMs can be used with correct prompting
to classify WOs.

\bigskip
Current research has demonstrated that transformer-based models
(a neural network architecture, meaning it learns by tracking relationships
between words \cite{merritt2022}),
such as BERT and other domain-specific variants, significantly improve text
classification processes through transfer learning and fine-tuning
\cite{nazyrova2024}.
While Nazyrova et al. focuses on medical text classification and our focus lies
within WO classification, their findings on the topic of fine-tuning LLMs is
relevant for our paper and its research.
