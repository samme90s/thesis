\section{Introduction}

This paper introduces several aspects of our research and presents its main scope. We describe how large language models (LLMs) and their advanced reasoning capabilities can be applied in text classification. In collaboration with UPTILT, a company that delivers applications to customers within the service business, we analyze real-world work order (WO) data that has been redacted to ensure anonymity and privacy.

By leveraging modern LLMs, which have evolved from traditional machine learning (ML) methods, we aim to discover the accuracy in classifying unstructured text between the two methods, but also to look at the efficiency in comparison. Our approach is fundamentally quantitative -- obtaining numerical measurements such as classification accuracy and F1-score through rigorous statistical tests. As Wohlin et al. \cite{wohlin2000software} argue, controlled experiments provide a clear framework for investigating cause-and-effect relationships by limiting external influences. This gives us confidence that any improvements in our text classification solution arise from the specific modifications we have introduced, rather than from uncontrolled factors.

... to be continued when most of the work is complete in the rest of the document...

\subsection{Background}

In many industries today, handling large quantities data is common. Text classification is one key application area where models are trained to classify data, or more specific, text documents into pre-defined classes. Traditionally, ML approaches have been used to build these classifiers. Early techniques depended on rule-based systems and simple algorithms like logistic regression or Naive Bayes, which relied on manually crafted features derived from structured data \cite{bing2011mining}. 

Data is generally divided into two main categories: structured and unstructured. \textbf{Structured data} is organized in a clear, predefined format. For example, consider an employee roster stored in a spreadsheet where each row represents an employee and columns indicate details such as name, employee ID, department, and phone number. Similarly, a financial report that lists company names, dates, and expense figures in a table is also structured data. Its orderly layout makes it easy for business users to read and analyze. \textbf{Unstructured data} lacks this uniform organization. Examples include emails, text documents, or social media posts, where information is presented in varying formats without a strict arrangement. Work orders are a typical example of unstructured data. A work order might include a lengthy description of maintenance tasks, lists of required materials, details on location and customer info, and other text that does not fit into neatly labeled columns \cite{ibm2023work}. Because each document can be formatted differently, unstructured data does not lend itself to simple, tabular representation \cite{ibm2025datadiff}.

However, the early techniques often fall short in capturing language nuances, context details, and the ambiguity in unstructured text. More recently, advances in natural language processing (NLP) have led to the development of large language models (LLMs) based on transformer architectures such as GPT and BERT. These models use self-attention to focus on important parts of the text and capture deep meaning by understanding relationships between words. There are even newer versions that add reasoning capabilities, which aim to improve decision-making in text classification. Learning from vast amounts of pre-trained data and use self-attention to track relationships between words, which significantly improves their ability to understand and predict next word context. For example, while traditional ML models focus on manually extracting features, modern LLMs such as the previously mentioned GPT and BERT can adapt to various types of text with minimal human intervention such as pre processing and structuring. Supporting this development, recent research has shown that LLMs -- especially when fine-tuned for particular domains -- exhibit not only advanced language understanding but also reasoning capabilities that can further improve text classification performance \cite{huang2024classification, andersson2024ikea, merritt2022transformer, nazyrova2024medical, wang2024classifiers}.

One of the most significant advantages of large language models over traditional approaches is their ability to generalize to new tasks using \textbf{zero-shot}, \textbf{one-shot}, and \textbf{few-shot} learning \cite{brown2020language}. These concepts refer to how much prior labeled data a model needs to perform a task. Zero-shot learning allows an LLM to classify text without being explicitly trained on any labeled examples. The model uses pre-trained knowledge to infer the most likely category based on a prompt describing the task. One-shot learning improves upon this by giving the model just one labeled example before making the classification. Finally, few-shot learning further enhances accuracy by providing multiple labeled examples in the prompt, allowing the model to generalize more effectively without requiring full retraining.

Switching focus to a machine learning technique such as recurrent neural networks or long short-term memory (LSTMs) shows capability in handling sequences of words and remembering long distances between related words. This makes it easier for the model to understand context without needing manual feature design. This technique was introduced in the late 90's and is far less complex and resource consuming than the latter transformer technique and is still considered a state-of-the-art algorithm \cite{wang2024classifiers, hochreiter1997long}. However, training and testing an RNN och LSTM model still requires a significant amount of resources when using a large dataset.

Although as Nazyrova et al. \cite{nazyrova2024medical} study focuses on the medical application area within text classification, the principles are directly applicable to our context.  Traditionally, companies rely on manual processing of WOs, a practice that is both labor-intensive and error prone \cite{li2024work}. In our study, we target a more complex problem: unlike a binary classification that simply decides "yes" or "no," our goal is to automatically classify these WOs into multiple distinct categories. This challenge calls for a solution that can handle rich, unstructured text information and assign precise labels based on the documentâ€™s content. By comparing reasoning or non-reasoning LLM with state-of-the-art ML algorithms we aim to provide a clearer picture if it is efficient with the former tool (RQ1, RQ2).

\subsection{Related work}

% https://link.springer.com/article/10.1007/s10115-024-02321-1#Sec4 (fake news or real news article classification)

% https://www.sciencedirect.com/science/article/pii/S0925231225002929#sec1 (movie production review notes classification)

% https://skemman.is/handle/1946/49119?locale=en (language models for classification of patient text messages)

Sahu \cite{sahu2025language} mentions in his conclusion that there is a clear advantage in using a large language model, particularly BERT-based, over the common machine learning methods such as support vector machines. Scores, such as F1, recall, precision and accuracy, was always better for LLMs in this regard and mentions how these result were validated by a one-vs-all significance test. Capturing semantic relationships within the text is where the LLMs shine compared to the traditional machine learning methods which rely on term frequency - inverse document frequency (TF-IDF) or bag-of-words resulting in a lack of deeper understanding with nuanced language. In his case this nuanced language was within the medical terminology scope. Since this is similar to our work where news articles and work orders often have a nuanced language based of who writes them and sometimes containing slang or abbreviations. We aim to provide better answers to this area and see if traditional machine learning methods such as support vector machines can perform on par with the large language models, but also investigate if a reasoning LLM could improve upon this with efficiency and cost in mind.

... Check other works here and position ourselves to their conclusions and discussions so we know what to build upon to provide a knowledge contribution...

\subsection{Research questions}

Based on our focus on text classification and its evolution, this thesis aims to answer the following research questions:

\bigskip
\textit{RQ1: Accuracy Comparison
\newline
How does the classification accuracy of reasoning LLMs, non-reasoning LLMs, and traditional ML algorithms compare when categorizing unstructured news articles?}

\bigskip
\textit{RQ2: Efficiency Analysis
\newline
What are the differences in cost and time efficiency among reasoning LLMs, non-reasoning LLMs, and traditional ML algorithms for this classification task?}

\bigskip
\textit{RQ3: Applicability to Hierarchical Classification
\newline
How effectively can reasoning LLMs, non-reasoning LLMs, and traditional ML algorithms classify unstructured news articles into a multi-level category and subcategory structure similar to work orders?}

\bigskip
\textit{RQ4: Demonstrate a case study with work orders on the identified best model from the previous steps} % (goals/aims)

% \bigskip
% \texit{RQ1: Is it efficient, based on accuracy, cost, and time, when classifying unstructured news articles within multiple categories and sub categories similar to the notation of work orders, using reasoning and non-reasoning LLM compared to traditional ML algorithms?}

% \bigskip
%\textit{RQ1: Is it efficient based on the accuracy when using a large language model for text classification of unstructured text data compared to a state-of-the-art machine learning model?}

% \bigskip
% \textit{RQ2: Can a higher accuracy be achieved when using reasoning large language models for text classification of unstructured data compared to a non-reasoning large language model or a state-of-the-art machine learning model?}