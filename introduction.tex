\section{Introduction}

This report introduces several aspects of our research and presents its main scope. We describe how large language models (LLMs) and their advanced reasoning capabilities can be applied in text classification. In collaboration with UPTILT, a company that delivers applications to customers within the service business, we analyze real-world work order (WO) data that has been redacted to ensure anonymity and privacy.

By leveraging modern LLMs, which have evolved from traditional machine learning (ML) methods, we aim to discover the accuracy in classifying unstructured text between the two methods (RQ1), but also to look at the efficiency in comparison (RQ2). Our approach is fundamentally quantitative -- obtaining numerical measurements such as classification accuracy and F1-score through rigorous statistical tests. As Wohlin et al. \cite{wohlin2000software} argue, controlled experiments provide a clear framework for investigating cause-and-effect relationships by limiting external influences. This gives us confidence that any improvements in our text classification solution arise from the specific modifications we have introduced, rather than from uncontrolled factors.

... to be continued when most of the work is complete in the rest of the document...

\subsection{Background and Motivation}

...

\subsection{Related work}

% https://link.springer.com/article/10.1007/s10115-024-02321-1#Sec4 (fake news or real news article classification)

% https://ieeexplore.ieee.org/abstract/document/10724558?casa_token=0d-_C3B0ezIAAAAA:QKkmkyulYbUoEcvwP8JJPzTo1T4BvLElpere82hqa3i1yzPX2NXOpP__4VzhDFFcCrEXJnK4-Q (Telugu news classification using various models including svm)

Sahu \cite{sahu2025language} mentions in the conclusion of his master's thesis that there is a clear advantage in using large language models, particularly BERT-based ones, over common machine learning methods such as support vector machines. Scores, such as F1, recall, precision, and accuracy, were always better for LLMs in this regard, and he mentions how these results were validated by a one-vs-all significance test. Capturing semantic relationships within the text is where the LLMs shine compared to traditional machine learning methods, which rely on term frequency-inverse document frequency (TF-IDF) or bag-of-words, resulting in a lack of deeper understanding with nuanced language. In his case, this nuanced language was within the medical terminology scope. Since this is similar to our work, where news articles and work orders often have nuanced language based on who writes them and sometimes contain slang or abbreviations, we aim to provide better answers in this area. We want to see if traditional machine learning methods, such as support vector machines, can perform on par with large language models, but also investigate if a reasoning LLM could improve upon this with efficiency and cost in mind.

Niraula et al. \cite{niraula2024multi} explored using both proprietary generative large language models (LLMs), including GPT-3.5, and specific open-source models like Mistral-7B for multi-label classification. Their work using these models on text with specialized language from the aviation domain, similar to the slang or abbreviations in our news articles and work orders, provides a relevant comparison point. While their focus was on applying these generative models and testing practical aspects like few-shot learning, our project aims to directly compare the performance of a traditional method: Support Vector Machine (SVM), against different LLM types. We can build upon the authors findings by evaluating not just the effectiveness of these generative models for our specific task, but also assessing their efficiency, cost, and data requirements relative to both SVM and other LLMs, helping to determine the most practical approach for our context.

Further supporting the potential of LLMs for complex text, Garcés et al. \cite{garces2025leveraging} found that transformer models, including both encoder-only (RoBERTa) and decoder-only (Llama 2) types, performed significantly better than SVM when classifying short review notes from animation production. These notes most likely contained slang and informal language, similar to the challenges we see in our news articles and work orders. While their results showed the clear performance benefits of LLMs in understanding such nuanced text, Garcés et al. also pointed out the significantly higher computational cost, especially regarding training time, for these advanced models compared to SVM. This highlights an important balance between performance and resource usage. Building on these findings, and similar explorations like those by Niraula et al. \cite{niraula2024multi}, our work aims to investigate this performance-cost trade-off within our specific context. We will directly compare SVM against selected LLMs: Mistral-7B, GPT-3.5, and reasoning models. The goal is to see if traditional methods can provide good enough results while being more resource-efficient, or if the performance gains from specific LLM architectures warrant their higher cost for classifying our nuanced news and work order texts.

\subsection{Research questions}

Based on our focus on text classification and its evolution, this thesis aims to answer the following research questions:

\bigskip
\textit{RQ1: Accuracy Comparison
\newline
How does the classification accuracy of reasoning LLMs, non-reasoning LLMs, and traditional ML algorithms compare when categorizing unstructured news articles?}

\bigskip
\textit{RQ2: Efficiency Analysis
\newline
What are the differences in cost and time efficiency among reasoning LLMs, non-\\reasoning LLMs, and traditional ML algorithms for this classification task?}

\bigskip
\textit{RQ3: Applicability to Hierarchical Classification
\newline
How effectively can reasoning LLMs, non-reasoning LLMs, and traditional ML algorithms classify unstructured news articles into a multi-level category and subcategory structure similar to work orders?}

\bigskip
\textit{RQ4: Demonstrate a case study with work orders on the identified best model from the previous steps} % (goals/aims)

In our study, we target a more complex problem: unlike a binary classification that simply decides "yes" or "no," our goal is to automatically classify these WOs into multiple distinct categories (RQ3). This challenge calls for a solution that can handle rich, unstructured text information and assign precise labels based on the document’s content. By comparing reasoning or non-reasoning LLM with state-of-the-art ML algorithms (RQ1, RQ2), we aim to provide a clearer picture of a suitable approach and demonstrate this both on our news article and work orders dataset (RQ4).

\subsection{Target group}
\subsection{Authors Contributions}