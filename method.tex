\section{Method}
% https://www.scribbr.com/dissertation/methodology/
% https://www.youtube.com/watch?v=2MqLFH7CalQ
%
% It should include:
%
% The type of research you conducted
% How you collected and analyzed your data
% Any tools or materials you used in the research
% How you mitigated or avoided research biases
% Why you chose these methods
%
% Your methodology section should generally be written in the PAST TENSE!
In our work on text classification, it was important to directly measure how changes in our methodological factors factors (independent variables) influenced performance metrics (dependent variables). Our primary focus was to assess the relative effectiveness of different model classes --- long short-term memory machine learning algorithms, non-reasoning large language models (LLM), and reasoning-enabled LLMs --- in classifying unstructured text data. That is why we chose a controlled experiment --- as it let us tweak one or more factors (for example, feature extraction or model parameters) while keeping all other conditions the same. This controlled setting made it easier to observe and measure the impact on key performance indicators, like classification accuracy or F1-score, using statistical tests. Because we obtained numerical measurements from our experiments, our approach is inherently quantitative.
Wohlin et al. \cite{wohlin2000} explain that controlled experiments provide a clear way to investigate cause-and-effect relationships. Compared to surveys or case studies --- which may mix in effects from uncontrolled factors --- a controlled experiment minimizes external influences. This allows us to be more confident that any changes in the performance of our text classification system are due to the specific modifications we introduced rather than random variation.

\subsection{Experiment design}
% Continuing with design and setup subsection...
% As example:
% - The machine used
% - The software used (versions!)
% - What types of measurement tools where used (again versions, unless it is already a part of the software)
% Add sources/references here to support our choices!
With our collaboration we got access an extensive dataset consisting of anonymized Work Orders (WOs) derived from genuine service business operations. Each WO represented unstructured textual information described earlier in the Background section, including task descriptions, instructions, lists of materials, and other relevant details without structured formatting.

Before starting the iterations of the experiment, we further cleaned the dataset and removed any sensitive information. A method such as tokenization and word segmentation was applied using natural language toolkit (NLTK) --- a standard toolkit found in Python and recommended by \cite{bird2009nlp}. This process involved breaking the text into tokens (words, punctuation marks, etc.) and accurately identifying word boundaries and was our first step in the process. Stop-word removal was applied using the same library, eliminating common words \textit{("the", "is", "at" and more...)} that does not contribute any significant meaning to the text when used in conjunction with ML and LLMs. The text was further cleaned from any punctuations and symbols.

To facilitate a fair and reproducible comparison across multiple configurations, the dataset was then randomly divided using a fixed-seed splitting method rooted in digits of pi into two distinct subsets: training (~80\%) and testing (~20\%). The training set was used to train (or in the case of LLMs, prompt-engineer and fine-tune prompts for) classification models, while the testing dataset was reserved strictly for their performance evaluation.

Since we did not have access to a domain expert from the specific service business area, prompt construction was based on available domain documentation and published industry terminology in similar fields. To ensure correctness and appropriateness, we systematically referred to standard terminology used in similar industry reports and publicly available domain-specific resources throughout the prompt development stage. Although involving a domain expert is highly recommended, our documentation of each iteration still provides reproducibility. However, the absence of expert validation remains a limitation of our approach.

% Specify the LLMs and ML models/algorithms used (e.g., GPT-4o, Deepseek, Naive Bayes).
%
% Explain why these models were chosen and how they are relevant to the research question.
% Describe whether the models are pre-trained, fine-tuned, or used with prompt engineering.
% Explain any hyperparameter tuning performed (e.g., learning rate, batch size).
% Discuss the computational resources required (e.g., GPUs, cloud services).
%
% Use references to support our choices here!
... discuss models/algorithms and their configurations here...
We chose to work with long short-term memory algorithms and the later GPT models from \textit{OpenAI} based of Wang et al. studies \cite{wang2024}.

\subsection{Variables}

% A variable is anything that can change or be changed => any factor that can be
% manipulated, controlled for, or measured in an experiment.
%
% Independent variables:
% These are factors that you can manipulate in an experiment.
% Our hypothesis is that respective independent variable causes a direct effect on the dependent variable(s).
\subsubsection{Independent}

\begin{itemize}
    \item Train \& Test subsets
    \item Models \& Algorithms
    \item Model configurations
    \item Prompt
\end{itemize}

% Dependent variables:
% These are factors that you can observe or measure. As we change the independent variable(s),
% we observe what happens to the dependent variable(s).
\subsubsection{Dependent}

\begin{itemize}
    \item ... fill out what we expect here from RAGAS and SciKit Learn...
\end{itemize}

% --------------------------------------------
\subsection{Data collection}
% Step 2:
% Methods of data collection:
%
% - The sampling method or critera
% - The tools, procedures, and materials
% - How variables were measured

\subsection{Data analysis}
% Step 3:
% Describe the methods of analysis (how you processed and analyzed the data)
%
% A rigorous skepticism should be a part of every experiment:
% - Is this really a correct (fair) comparison?
% - Can we really draw these conclusions based on this experiment?
% - Have we taken all independent variables into account?
% - Are the measurements correct?

\subsection{Research biases (reliability and validity)}
% Step 4:
% Evaluate and justify methodological choices
%
% Why you chose these particular methods?
% Why other methods were not suitable for your objectives?
% How this approach contributes new knowledge or understanding?