\section{Method}
% https://www.scribbr.com/dissertation/methodology/
% https://www.youtube.com/watch?v=2MqLFH7CalQ
%
% It should include:
%
% The type of research you conducted
% How you collected and analyzed your data
% Any tools or materials you used in the research
% How you mitigated or avoided research biases
% Why you chose these methods
%
% Your methodology section should generally be written in the PAST TENSE!
In our work on text classification, it was important to directly measure how changes in our methodological factors factors (independent variables) influenced performance metrics (dependent variables). Our primary focus was to assess the relative effectiveness of different model classes --- long short-term memory machine learning algorithms, non-reasoning large language models (LLM), and reasoning-enabled LLMs --- in classifying unstructured text data. That is why we chose a controlled experiment --- as it let us tweak one or more factors (for example, feature extraction or model parameters) while keeping all other conditions the same. This controlled setting made it easier to observe and measure the impact on key performance indicators, like classification accuracy or F1-score, using statistical tests. Because we obtained numerical measurements from our experiments, our approach is inherently quantitative.
Wohlin et al. \cite{wohlin2000software} explain that controlled experiments provide a clear way to investigate cause-and-effect relationships. Compared to surveys or case studies --- which may mix in effects from uncontrolled factors --- a controlled experiment minimizes external influences. This allows us to be more confident that any changes in the performance of our text classification system are due to the specific modifications we introduced rather than random variation.

\subsection{Experiment design}
% Continuing with design and setup subsection...
% As example:
% - The machine used
% - The software used (versions!)
% - What types of measurement tools where used (again versions, unless it is already a part of the software)
% Add sources/references here to support our choices!
With our collaboration we got access an extensive dataset consisting of anonymized Work Orders (WOs) derived from genuine service business operations. Each WO represented unstructured textual information described earlier in the Background section, including task descriptions, instructions, lists of materials, and other relevant details without structured formatting.

Before starting the iterations of the experiment, we further cleaned the dataset and removed any sensitive information. A method such as tokenization and word segmentation was applied using natural language toolkit (NLTK) --- a standard toolkit found in Python and recommended by \cite{bird2009nlp}. This process involved breaking the text into tokens (words, punctuation marks, etc.) and accurately identifying word boundaries and was our first step in the process. Stop-word removal was applied using the same library, eliminating common words \textit{("the", "is", "at" and more...)} that does not contribute any significant meaning to the text when used in conjunction with ML and LLMs. The text was further cleaned from any punctuations and symbols.

To facilitate a fair and reproducible comparison across multiple configurations, the dataset was then randomly divided using a fixed-seed splitting method rooted in digits of pi into two distinct subsets: training (~80\%) and testing (~20\%). The training set was used to train (or in the case of LLMs, prompt-engineer and fine-tune prompts for) classification models, while the testing dataset was reserved strictly for their performance evaluation.

Since we did not have access to a domain expert from the specific service business area, prompt construction was based on available domain documentation and published industry terminology in similar fields. To ensure correctness and appropriateness, we systematically referred to standard terminology used in similar industry reports and publicly available domain-specific resources throughout the prompt development stage. Although involving a domain expert is highly recommended, our documentation of each iteration still provides reproducibility. However, the absence of expert validation remains a limitation of our approach.

% Specify the LLMs and ML models/algorithms used (e.g., GPT-4o, Deepseek, Naive Bayes).
%
% Explain why these models were chosen and how they are relevant to the research question.
% Describe whether the models are pre-trained, fine-tuned, or used with prompt engineering.
% Explain any hyperparameter tuning performed (e.g., learning rate, batch size).
% Discuss the computational resources required (e.g., GPUs, cloud services).
%
% Use references to support our choices here!
For text classification, we selected long short-term memory (LSTM) algorithms and the later generative pretrained transformer (GPT) models based on their demonstrated effectiveness in prior research. Traditional machine learning (ML) models such as logistic regression, random forests, and naive bayes have been widely applied in text classification but typically require features and extensive labeled data to perform well. While these models have been useful, they struggle with capturing sequential dependencies and handling domain shifts effectively \cite{sarker2021machine}.

Deep learning approaches (recurrent neural networks (RNNs) and long short term memory algorithms) have shown state-of-the-art performance by capturing long-range dependencies in text \cite{sutskever2014sequence}. LSTMs, in particular, are known for their ability to mitigate vanishing gradient issues that affect standard RNNs, making them well-suited for text classification tasks \cite{hochreiter1997long}. Despite their advantages, LSTMs still require substantial labeled datasets to generalize across different domains effectively. Recent work has demonstrated that fine-tuning or introducing few-shot learning can significantly improve their adaptability \cite{jamshidi2024effective}.

In contrast, large language models (LLMs) based on the Transformer architecture, such as GPT-4 from \textit{OpenAI} and LLaMA from \textit{Meta}, leverages pre training on many diverse datasets, enabling effective zero-shot and few-shot learning \cite{brown2020language, touvron2023llama}. Studies have consistently shown that LLMs outperform traditional machine learning and neural network based classifiers across various text classification tasks, including sentiment analysis, economic text classification, and spam detection \cite{moller2024parrot, betianu2024dallmi}. Fine-tuned LLMs achieve even higher classification accuracy and often surpasses deep learning models. Additionally, LLMs simplify classification pipelines by eliminating the need for extensive preprocessing and handcrafted feature engineering \cite{oh2024language}.

Given these findings, we chose LSTMs for their proven effectiveness in sequence modeling and GPT-based models for their adaptability and few-shot classification capabilities. By choosing both LSTMs and LLMs, we can compare how well LSTMs handle structured word sequences versus how LLMs can adapt to different text classification tasks without needing extensive training and answer both of our research questions \textit{(RQ1, RQ2)}.

\subsection{Variables}
A variable is anything that can change or be changed—any factor that can be manipulated, controlled for, or measured in an experiment. In this study, we distinguish between two main types of variables: independent variables, which we manipulate to observe their effect, and dependent variables, which we measure to capture the outcomes.

Our independent variables include the choice of train and test subsets, the models and algorithms used, the specific model configurations, and the prompt. We hypothesize that each of these factors can directly influence the results. To assess their effects, we rely on our dependent variables, namely accuracy, precision, recall, and the F1-score. By measuring these metrics under various settings, we can compare and quantify how each independent variable impacts overall performance.

The four dependent variables is a combination of results from classifications.  A true positive (TP) means the predicted label matches the true label in contrast  to the true negative (TN), which means the predicted label is “negative” and the  actual label is also "negative." Both TPs and TNs represent accurate predictions,  whereas a false positive (FP) or false negative (FN) indicates that the  classification was incorrect.

\medskip
Accuracy is the percentage of correct predictions over the total number of  predictions. More concretely, it measures how often the model labels samples  correctly compared to the total number of samples evaluated, and can be written  as: where $\mathrm{TP}$ is the number of true positives, $\mathrm{TN}$ is the  number of true negatives, $\mathrm{FP}$ is the number of false positives,  and $\mathrm{FN}$ is the number of false negatives.

$$
\mathrm{Accuracy} = \frac{\mathrm{TP} + \mathrm{TN}}
{\mathrm{TP} + \mathrm{TN} + \mathrm{FP} + \mathrm{FN}}
$$

\medskip
Precision displays the model’s accuracy in predicting a particular label (also known as class).  In other words, it measures how often the model is correct when it predicts that label.  Was calculated as follows: where $\mathrm{TP}$ is the number of true positives and $\mathrm{FP}$ is the  number of false positives.

$$
\mathrm{Precision} 
= \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP}}
$$

\medskip
Recall can be seen as the ratio of positively recognized samples to all samples  in the actual label -- $\mathrm{TP}$ and $\mathrm{FN}$, where $\mathrm{TP}$ denotes the number of true positives, and $\mathrm{FN}$ represents the number of false negatives.

$$
\mathrm{Recall} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}}
$$

\medskip
F1 Score is the harmonic mean between Precision and Recall. More concretely, it is a single metric  that balances the trade-off between both metrics, especially useful when we need a combined measure.  Was calculated as follows: where $\mathrm{Precision}$ is the fraction of predicted positives that are  truly positive, and $\mathrm{Recall}$ is the fraction of actual positives that are predicted positive.

$$
\mathrm{F1} = \frac{2 \times \mathrm{Precision} \times \mathrm{Recall}}
{\mathrm{Precision} + \mathrm{Recall}}
$$

% --------------------------------------------
\subsection{Data collection}
% Step 2:
% Methods of data collection:
%
% - The sampling method or critera
% - The tools, procedures, and materials
% - How variables were measured

\subsection{Data analysis}
% Step 3:
% Describe the methods of analysis (how you processed and analyzed the data)
%
% A rigorous skepticism should be a part of every experiment:
% - Is this really a correct (fair) comparison?
% - Can we really draw these conclusions based on this experiment?
% - Have we taken all independent variables into account?
% - Are the measurements correct?

\subsection{Research biases (reliability and validity)}
% Step 4:
% Evaluate and justify methodological choices
%
% Why you chose these particular methods?
% Why other methods were not suitable for your objectives?
% How this approach contributes new knowledge or understanding?